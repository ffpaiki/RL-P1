{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The state location in the maze are: \n",
      "[[1 1]\n",
      " [1 2]\n",
      " [1 3]\n",
      " [1 4]\n",
      " [2 1]\n",
      " [2 3]\n",
      " [2 4]\n",
      " [3 1]\n",
      " [3 3]\n",
      " [3 4]\n",
      " [4 1]\n",
      " [5 1]\n",
      " [5 2]\n",
      " [5 3]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_states = 14\n",
    "num_actions = 4\n",
    "gamma = 1\n",
    "theta = 0.5\n",
    "prob = 0.1\n",
    "\n",
    "a_desc = [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"]\n",
    "# 0 - UP\n",
    "# 1 - DOWN\n",
    "# 2 - LEFT\n",
    "# 3 - RIGHT\n",
    "\n",
    "maze = [['W','W','W','W','W','W'],\n",
    "        ['W','S','S','S','S','W'],\n",
    "        ['W','S','W','B','S','W'],\n",
    "        ['W','S','W','S','B','W'],\n",
    "        ['W','S','W','W','G','W'],\n",
    "        ['W','B','S','S','W','W'],\n",
    "        ['W','W','W','W','W','W']]\n",
    "\n",
    "s_loc = np.zeros((num_states,2))\n",
    "# print(s_loc)\n",
    "\n",
    "# Initialize the state-value function V_0(s) = 0\n",
    "v = np.zeros(num_states)\n",
    "\n",
    "# Initialize the reward function\n",
    "r = np.zeros((num_actions,num_states, num_states))\n",
    "\n",
    "# Initialize the transition probability function\n",
    "p = np.zeros((num_actions,num_states, num_states))\n",
    "\n",
    "# Initialize the probability of taking each action in each state\n",
    "# pi = np.ones((num_states, num_actions, num_states))\n",
    "\n",
    "maze_col = len(maze[0])\n",
    "maze_row = len(maze)\n",
    "\n",
    "\n",
    "# Get the reward for the next state (s_prime)\n",
    "def getReward(s_prime):\n",
    "    reward = 0\n",
    "    \n",
    "    if(s_prime == 'B'):\n",
    "        reward += -10\n",
    "    elif(s_prime == 'G'):\n",
    "        reward += 200\n",
    "    \n",
    "    reward += -1\n",
    "    \n",
    "    return  reward\n",
    "\n",
    "# Map between the state and the location in the maze\n",
    "def mapStateToMaze():\n",
    "    s = 0\n",
    "    for i in range(maze_row):\n",
    "        for j in range(maze_col):\n",
    "            if(maze[i][j] == 'S' or maze[i][j] == 'B'):\n",
    "                s_loc[s] = [i,j]\n",
    "                s += 1\n",
    "\n",
    "mapStateToMaze()\n",
    "s_loc = s_loc.astype(int)\n",
    "\n",
    "print(\"The state location in the maze are: \")\n",
    "print(s_loc)\n",
    "\n",
    "# Get state from the location in the maze\n",
    "def getStateFromMaze(i,j):\n",
    "    k = np.where((s_loc == (i,j)).all(axis=1))\n",
    "    if(k[0].size == 0):\n",
    "        return 0\n",
    "    else:\n",
    "        return k[0][0]\n",
    "            \n",
    "# Get the next state given the current state and the action\n",
    "def getNextState(s, a):\n",
    "    s_prime = 0\n",
    "    \n",
    "    # The location of the current state in the maze\n",
    "    i = s_loc[s,0]\n",
    "    j = s_loc[s,1]\n",
    "    \n",
    "    if(a == 0):\n",
    "        # Action is UP\n",
    "        if(maze[i-1][j] != 'W'):\n",
    "            s_prime = getStateFromMaze(i-1,j)\n",
    "            p[a][s][s_prime] = 1 - prob\n",
    "        else:\n",
    "            s_prime = s\n",
    "    elif(a == 1):\n",
    "        # Action is DOWN\n",
    "        if(maze[i+1][j] != 'W'):\n",
    "            s_prime = getStateFromMaze(i+1,j)\n",
    "            p[a][s][s_prime] = 1 - prob\n",
    "        else:\n",
    "            s_prime = s\n",
    "    elif(a == 2):\n",
    "        # Action is LEFT\n",
    "        if(maze[i][j-1] != 'W'):\n",
    "            s_prime = getStateFromMaze(i,j-1)\n",
    "            p[a][s][s_prime] = 1 - prob\n",
    "        else:\n",
    "            s_prime = s\n",
    "    elif(a == 3):\n",
    "        # Action is RIGHT\n",
    "        if(maze[i][j+1] != 'W'):\n",
    "            s_prime = getStateFromMaze(i,j+1)\n",
    "            p[a][s][s_prime] = 1 - prob\n",
    "        else:\n",
    "            s_prime = s\n",
    "    \n",
    "    return s_prime\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in range(num_actions):\n",
    "    for s in range(num_states):\n",
    "        for s_prime in range(num_states):\n",
    "            r[a][s][s_prime] = getReward(maze[s_loc[s_prime,0]][s_loc[s_prime,1]])\n",
    "            # p[a][s][s_prime] = \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "V_{k+1} = \\max_{a \\in A} \\sum_{S'} P(S'|S,a) \\ \\ \\ [R(S,a,S')+V_k]\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1896551829.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "def getValueState(s, v, pi):\n",
    "    \n",
    "    # For each action\n",
    "    for a in range(num_actions):\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "V^{\\pi}(S) = \\sum_{S'} P(S'|S,\\pi(S)) \\ \\ \\ [R(S,\\pi(S),S')+V^{\\pi}(S')]\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "policy = np.zeros(num_states)\n",
    "\n",
    "print(policy)\n",
    "\n",
    "def policy_evaluation(env, policy, gamma=0.9, theta=0.01):\n",
    "    \"\"\"\n",
    "    Evaluates a given policy in a dynamic programming setting.\n",
    "\n",
    "    Args:\n",
    "        env: The environment with a defined MDP (states, actions, rewards, transitions)\n",
    "        policy: The policy to be evaluated (mapping of states to actions)\n",
    "        gamma: The discount factor for future rewards.\n",
    "        theta: The convergence threshold.\n",
    "\n",
    "    Returns:\n",
    "        np.array: An array of state-value estimates.\n",
    "    \"\"\"\n",
    "\n",
    "    # num_states = env.nS  # Number of states\n",
    "    value_func = np.zeros(num_states)  # Initialize state-value function\n",
    "\n",
    "    while True:\n",
    "        delta = 0  # Track the maximum change in value function\n",
    "        for s in range(num_states):\n",
    "            v = 0\n",
    "            action = policy[s]\n",
    "            for prob, next_state, reward, _ in env.P[s][action]:\n",
    "                v += prob * (reward + gamma * value_func[next_state])\n",
    "            delta = max(delta, abs(v - value_func[s]))\n",
    "            value_func[s] = v  \n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    return value_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
