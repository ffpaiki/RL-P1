{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T23:24:12.660620Z",
     "start_time": "2024-02-28T23:24:12.363124Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The state location in the maze are: \n",
      "[[1 1]\n",
      " [1 2]\n",
      " [1 3]\n",
      " [1 4]\n",
      " [2 1]\n",
      " [2 3]\n",
      " [2 4]\n",
      " [3 1]\n",
      " [3 3]\n",
      " [3 4]\n",
      " [4 1]\n",
      " [4 4]\n",
      " [5 1]\n",
      " [5 2]\n",
      " [5 3]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "num_states = 15\n",
    "num_actions = 4\n",
    "\n",
    "prob = 0.02\n",
    "gamma = 0.95\n",
    "theta = 0.05\n",
    "\n",
    "a_desc = [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"]\n",
    "# 0 - UP\n",
    "# 1 - DOWN\n",
    "# 2 - LEFT\n",
    "# 3 - RIGHT\n",
    "\n",
    "maze = [['W','W','W','W','W','W'],\n",
    "        ['W','S','S','S','S','W'],\n",
    "        ['W','S','W','B','S','W'],\n",
    "        ['W','S','W','S','B','W'],\n",
    "        ['W','S','W','W','G','W'],\n",
    "        ['W','B','S','S','W','W'],\n",
    "        ['W','W','W','W','W','W']]\n",
    "\n",
    "s_loc = np.zeros((num_states,2))\n",
    "\n",
    "# Initialize the reward function\n",
    "r = np.zeros((num_actions,num_states, num_states))\n",
    "\n",
    "# Initialize the transition probability function\n",
    "p = np.zeros((num_actions,num_states, num_states))\n",
    "\n",
    "# Initialize the probability of taking each action in each state\n",
    "# pi = np.ones((num_states, num_actions, num_states))\n",
    "\n",
    "maze_col = len(maze[0])\n",
    "maze_row = len(maze)\n",
    "\n",
    "\n",
    "# Get the reward for the next state (s_prime)\n",
    "def getReward(s_prime):\n",
    "    reward = 0\n",
    "    \n",
    "    # The location of the current state in the maze\n",
    "    i = s_loc[s_prime,0]\n",
    "    j = s_loc[s_prime,1]\n",
    "    \n",
    "    if(maze[i][j] == 'B'):\n",
    "        reward += -10\n",
    "    elif(maze[i][j] == 'G'):\n",
    "        reward += 200\n",
    "    \n",
    "    reward += -1\n",
    "    \n",
    "    return  reward\n",
    "\n",
    "# Map between the state and the location in the maze\n",
    "def mapStateToMaze():\n",
    "    s = 0\n",
    "    for i in range(maze_row):\n",
    "        for j in range(maze_col):\n",
    "            if(maze[i][j] == 'S' or maze[i][j] == 'B' or maze[i][j] == 'G'):\n",
    "                s_loc[s] = [i,j]\n",
    "                s += 1\n",
    "\n",
    "mapStateToMaze()\n",
    "s_loc = s_loc.astype(int)\n",
    "\n",
    "print(\"The state location in the maze are: \")\n",
    "print(s_loc)\n",
    "\n",
    "# Get state from the location in the maze\n",
    "def getStateFromMaze(i,j):\n",
    "    k = np.where((s_loc == (i,j)).all(axis=1))\n",
    "    if(k[0].size == 0):\n",
    "        return 0\n",
    "    else:\n",
    "        return k[0][0]\n",
    "            \n",
    "# Get the next state given the current state and the action\n",
    "def getNextState(s, a):\n",
    "    s_prime = 0\n",
    "    \n",
    "    # The location of the current state in the maze\n",
    "    i = s_loc[s,0]\n",
    "    j = s_loc[s,1]\n",
    "    \n",
    "    if(a == 0):\n",
    "        # Action is UP\n",
    "        if(maze[i-1][j] != 'W'):\n",
    "            s_prime = getStateFromMaze(i-1,j)\n",
    "        else:\n",
    "            s_prime = s\n",
    "    elif(a == 1):\n",
    "        # Action is DOWN\n",
    "        if(maze[i+1][j] != 'W'):\n",
    "            s_prime = getStateFromMaze(i+1,j)\n",
    "        else:\n",
    "            s_prime = s\n",
    "    elif(a == 2):\n",
    "        # Action is LEFT\n",
    "        if(maze[i][j-1] != 'W'):\n",
    "            s_prime = getStateFromMaze(i,j-1)\n",
    "        else:\n",
    "            s_prime = s\n",
    "    elif(a == 3):\n",
    "        # Action is RIGHT\n",
    "        if(maze[i][j+1] != 'W'):\n",
    "            s_prime = getStateFromMaze(i,j+1)\n",
    "        else:\n",
    "            s_prime = s\n",
    "    \n",
    "    # Insert the reward of the going from s to s_prime by taking action a\n",
    "    if(s != 11):\n",
    "        r[a][s][s_prime] = getReward(s_prime)\n",
    "    else:\n",
    "        r[a][s][s_prime] = 0\n",
    "        \n",
    "    return s_prime\n",
    "\n",
    "getAdjacentStates = lambda s: [getNextState(s,0), getNextState(s,1), getNextState(s,2), getNextState(s,3)]\n",
    "\n",
    "rewardMatrix = lambda s: [getReward(s_prime) for s_prime in getAdjacentStates(s)]\n",
    "\n",
    "# Calculate the probability of the other states given the current state and action\n",
    "def getOtherState(s,a):\n",
    "    for i in range(num_actions):\n",
    "        s_prime = getNextState(s,i)\n",
    "        if(i != a):\n",
    "            p[a][s][s_prime] += prob/3\n",
    "        # else:\n",
    "        #     p[a][s][s_prime] = prob/3\n",
    "\n",
    "\n",
    "# Create the probability transition matrix\n",
    "def createTransitionMatrix(prob):\n",
    "    for a in range(num_actions):\n",
    "        for s in range(num_states):\n",
    "            next_state = getNextState(s,a)\n",
    "            adjStates = getAdjacentStates(s)\n",
    "\n",
    "            for s_prime in range(len(adjStates)):\n",
    "                if(adjStates[s_prime] == next_state):\n",
    "                    p[a][s][adjStates[s_prime]] = 1-prob\n",
    "                    getOtherState(s,a)\n",
    "                # else:\n",
    "                #     p[s_prime][s][adjStates[s_prime]] = prob/3\n",
    "\n",
    "\n",
    "\n",
    "def createRewardMatrix():\n",
    "    for a in range(num_actions):\n",
    "        for s in range(num_states):\n",
    "            adjStates = getAdjacentStates(s)\n",
    "            for s_prime in adjStates:\n",
    "                r[a][s][s_prime] = rewardMatrix(s)[a]\n",
    "\n",
    "createTransitionMatrix(prob)\n",
    "# print(\"\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "\n",
    "## Policy Evaluation\n",
    "\n",
    "Value-State Function:\n",
    "\n",
    "$\n",
    "V^{\\pi}(S) = \\sum_{S'} P(S'|S,\\pi(S))[R(S,\\pi(S),S')+\\gamma V^{\\pi}(S')]\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T23:23:44.530265Z",
     "start_time": "2024-02-28T23:23:44.512925Z"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 105\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28mprint\u001b[39m(pi)\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pi, policy_stable\n\u001b[0;32m--> 105\u001b[0m v, pi \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpi_0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# print(\"The value function is: \")\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# print(v)\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# print(\"The optimal policy is: \")\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# print(pi)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 20\u001b[0m, in \u001b[0;36mpolicy_iteration\u001b[0;34m(p, r, gamma, theta, pi_0)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m(policy_stable \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     19\u001b[0m     v, delta \u001b[38;5;241m=\u001b[39m policy_evaluation(p,r,gamma,theta,pi)\n\u001b[0;32m---> 20\u001b[0m     pi, policy_stable \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy_improvement\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m v, pi\n",
      "Cell \u001b[0;32mIn[6], line 73\u001b[0m, in \u001b[0;36mpolicy_improvement\u001b[0;34m(p, r, gamma, v, pi)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;28mlen\u001b[39m(v_index) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(v_index)):\n\u001b[0;32m---> 73\u001b[0m         next_state \u001b[38;5;241m=\u001b[39m getNextState(s,\u001b[43mv_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43mm\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;66;03m# The location of the current state in the maze\u001b[39;00m\n\u001b[1;32m     76\u001b[0m         i \u001b[38;5;241m=\u001b[39m s_loc[next_state,\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "prob = 0\n",
    "gamma = 0.95\n",
    "theta = 0.5\n",
    "    \n",
    "# Initialize the transition probability function\n",
    "p = np.zeros((num_actions,num_states, num_states))\n",
    "\n",
    "createTransitionMatrix(prob)\n",
    "createRewardMatrix()\n",
    "\n",
    "pi_0 = np.zeros((num_states)).astype(int)\n",
    "pi_0 += 2\n",
    "\n",
    "\n",
    "def policy_iteration(p, r, gamma, theta, pi_0):\n",
    "    pi = pi_0\n",
    "    policy_stable = False\n",
    "    while(policy_stable == False):\n",
    "        v, delta = policy_evaluation(p,r,gamma,theta,pi)\n",
    "        pi, policy_stable = policy_improvement(p,r,gamma,v,pi)\n",
    "    return v, pi\n",
    "\n",
    "def policy_evaluation(p, r, gamma, theta, initial_pi):\n",
    "    v_0 = np.zeros((num_states))\n",
    "    v_1 = np.zeros((num_states))\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        v_0 = np.copy(v_1)\n",
    "        v_1 = np.zeros((num_states))\n",
    "\n",
    "        for s in range(num_states):\n",
    "            # v_0[s] = v_1[s]\n",
    "            a = initial_pi[s]\n",
    "            \n",
    "            # v_1[s] = 0\n",
    "            \n",
    "            for s_prime in range(num_states):\n",
    "                if(s != 11 or s_prime != 11):\n",
    "                    if(p[a][s][s_prime] != 0):\n",
    "                        v_1[s] += (p[a][s][s_prime] * (r[a][s][s_prime] + (gamma * v_0[s_prime])))\n",
    "                    else:\n",
    "                        v_1[s] += 0\n",
    "            \n",
    "            v_1[11] =0\n",
    "            \n",
    "\n",
    "        for s in range(num_states):\n",
    "            delta = max(delta, abs(v_0[s] - v_1[s]))\n",
    "                \n",
    "            if(delta < theta):\n",
    "                return v_1,delta\n",
    "        # print(v_1)\n",
    "\n",
    "\n",
    "# Generate method for policy improvement in reinforcement learning\n",
    "def policy_improvement(p, r, gamma, v, pi):\n",
    "    policy_stable = True\n",
    "    for s in range(num_states):\n",
    "        old_action = pi[s]\n",
    "        q = np.zeros((num_actions))\n",
    "        \n",
    "        for a in range(num_actions):\n",
    "            for s_prime in range(num_states):\n",
    "                if(p[a][s][s_prime] != 0):\n",
    "                    q[a] += (p[a][s][s_prime] * (r[a][s][s_prime] + (gamma * v[s_prime])))\n",
    "                else:\n",
    "                    q[a] += 0\n",
    "        \n",
    "        v_index = np.argwhere(q == np.max(q)).flatten().tolist()\n",
    "        if(len(v_index) > 1):\n",
    "            for m in range(len(v_index)):\n",
    "                next_state = getNextState(s,v_index[m])\n",
    "\n",
    "                # The location of the current state in the maze\n",
    "                i = s_loc[next_state,0]\n",
    "                j = s_loc[next_state,1]\n",
    "\n",
    "                if(v_index[m] == 0):\n",
    "                    # Action is UP\n",
    "                    if(maze[i-1][j] == 'W'):\n",
    "                        v_index.pop(m)\n",
    "                elif(v_index[m] == 1):\n",
    "                    # Action is DOWN\n",
    "                    if(maze[i+1][j] == 'W'):\n",
    "                        v_index.pop(m)\n",
    "                elif(v_index[m] == 2):\n",
    "                    # Action is LEFT\n",
    "                    if(maze[i][j-1] == 'W'):\n",
    "                        v_index.pop(m)\n",
    "                elif(v_index[m] == 3):\n",
    "                    # Action is RIGHT\n",
    "                    if(maze[i][j+1] == 'W'):\n",
    "                        v_index.pop(m)\n",
    "        else:\n",
    "            pi[s] = np.argmax(q)\n",
    "        \n",
    "        if(old_action != pi[s]):\n",
    "            policy_stable = False\n",
    "            \n",
    "    print(v)\n",
    "    print(pi)\n",
    "    return pi, policy_stable\n",
    "\n",
    "v, pi = policy_iteration(p, r, gamma, theta, pi_0)\n",
    "\n",
    "# print(\"The value function is: \")\n",
    "# print(v)\n",
    "# print(\"The optimal policy is: \")\n",
    "# print(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration Backup\n",
    "\n",
    "$\n",
    "V_{k+1}(S) = max_{a \\in A} \\sum_{S'} \\; P(S'|S,a) \\; [R(S,a,S')+ \\gamma \\; V_{k}(S')]\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1. 199.  -1.   0.  -1.  -1.\n",
      "  -1.]\n",
      "[ -2.  -2.  -2.  -2.  -2.  -2. 188.  -2. 188. 199.  -2.   0.  -2.  -2.\n",
      "  -2.]\n",
      "[ -3.  -3.  -3. 187.  -3. 187. 188.  -3. 188. 199.  -3.   0.  -3.  -3.\n",
      "  -3.]\n",
      "[ -4.  -4. 186. 187.  -4. 187. 188.  -4. 188. 199.  -4.   0.  -4.  -4.\n",
      "  -4.]\n",
      "[ -5. 185. 186. 187.  -5. 187. 188.  -5. 188. 199.  -5.   0.  -5.  -5.\n",
      "  -5.]\n",
      "[184. 185. 186. 187. 183. 187. 188. 182. 188. 199. 181.   0. 180. 169.\n",
      " 168.]\n",
      "[184. 185. 186. 187. 183. 187. 188. 182. 188. 199. 181.   0. 180. 169.\n",
      " 168.]\n",
      "0 3\n",
      "1 3\n",
      "2 3\n",
      "3 1\n",
      "4 0\n",
      "5 1\n",
      "6 1\n",
      "7 0\n",
      "8 3\n",
      "9 1\n",
      "10 0\n",
      "11 0\n",
      "12 0\n",
      "13 2\n",
      "14 2\n"
     ]
    }
   ],
   "source": [
    "prob = 0\n",
    "gamma = 1\n",
    "theta = 0.5\n",
    "\n",
    "# Initialize the transition probability function\n",
    "p = np.zeros((num_actions,num_states, num_states))\n",
    "v_result = np.zeros((num_states))\n",
    "\n",
    "createTransitionMatrix(prob)\n",
    "createRewardMatrix()\n",
    "\n",
    "def vib(p, r, gamma, theta):\n",
    "    v_0 = np.zeros((num_actions, num_states))\n",
    "    v_1 = np.zeros((num_actions, num_states))\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        v_0 = np.copy(v_1)\n",
    "        v_1 = np.zeros((num_actions, num_states))\n",
    "\n",
    "        for s in range(num_states):\n",
    "            for a in range(num_actions):\n",
    "        \n",
    "                # v_1[a][s] = 0\n",
    "\n",
    "                for s_prime in range(num_states):\n",
    "                    if(s != 11 or s_prime != 11):\n",
    "                        if(p[a][s][s_prime] != 0):\n",
    "                            v_1[a][s] += (p[a][s][s_prime] * (r[a][s][s_prime] + (gamma * v_result[s_prime])))\n",
    "                        else:\n",
    "                            v_1[a][s] += 0\n",
    "        \n",
    "            # Find the maximum value state for all actions\n",
    "            v_result[s] = np.max(v_1[:,s])\n",
    "            \n",
    "            # Always set the value of the goal state to 0\n",
    "            v_result[11] = 0\n",
    "\n",
    "            for i in range(num_actions):\n",
    "                delta = max(delta, abs(v_0[i][s] - v_1[i][s]))\n",
    "            \n",
    "            if(delta < theta):\n",
    "                return v_result,v_0,delta\n",
    "        # print(v_0)\n",
    "        print(v_result)\n",
    "\n",
    "\n",
    "v_result,q,delta = vib(p,r,gamma,theta)\n",
    "# print(v_result)\n",
    "\n",
    "for i in range(num_states):\n",
    "    print(i,np.argmax(q[:,i]))\n",
    "    # print(np.max(q[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = [['W','W','W','W','W','W'],\n",
    "        ['W','S','S','S','S','W'],\n",
    "        ['W','S','W','B','S','W'],\n",
    "        ['W','S','W','S','B','W'],\n",
    "        ['W','S','W','W','G','W'],\n",
    "        ['W','B','S','S','W','W'],\n",
    "        ['W','W','W','W','W','W']]\n",
    "\n",
    "# This is a random matrix for example purposes. \n",
    "# Matrix is defined as 20x20 instead of 18x18 stated in the project description in order to treat borders as wall states\n",
    "State_Matrix = \\\n",
    "    np.array([[9,9,9,9,9,9],\n",
    "              [9,0,0,0,0,9],\n",
    "              [9,0,9,1,0,9],\n",
    "              [9,0,9,0,1,9],\n",
    "              [9,0,9,9,7,9],\n",
    "              [9,1,0,0,9,9],\n",
    "              [9,9,9,9,9,9]])\n",
    "        \n",
    "\n",
    "\n",
    "plt.subplots(figsize=(10,7.5))\n",
    "heatmap = sns.heatmap(State_Matrix, fmt='', linewidths=0.25, linecolor='black', cbar= False, cmap= 'rocket_r', vmax=9, vmin=0)\n",
    "heatmap.set_facecolor('black') # Color for the NaN cells in the state matrix\n",
    "plt.title('Maze Problem')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Function to always color the oil, bump, start, and green blocks.\n",
    " States are in the form of a list of (i,j) coordinates on the state matrix\"\"\"\n",
    "def coloring_blocks(heatmap, oil_states, bump_states, start_state, end_state):\n",
    "    # Adding red oil blocks\n",
    "    for i in range(len(oil_states)):\n",
    "        heatmap.add_patch(Rectangle((oil_states[i][1], oil_states[i][0]), 1, 1,\n",
    "                                    fill=True, facecolor='red', edgecolor='red', lw=0.25))\n",
    "    # Adding salmon bump blocks\n",
    "    for i in range(len(bump_states)):\n",
    "        heatmap.add_patch(Rectangle((bump_states[i][1], bump_states[i][0]), 1, 1,\n",
    "                                    fill=True, facecolor='lightsalmon', edgecolor='lightsalmon', lw=0.25))\n",
    "    # Adding start block (Blue)\n",
    "    heatmap.add_patch(Rectangle((start_state[1], start_state[0]), 1, 1,\n",
    "                                fill=True, facecolor='lightblue', edgecolor='lightblue', lw=0.25))\n",
    "\n",
    "    # Adding end block (Green)\n",
    "    heatmap.add_patch(Rectangle((end_state[1], end_state[0]), 1, 1,\n",
    "                                fill=True, facecolor='lightgreen', edgecolor='lightgreen', lw=0.25))\n",
    "\n",
    "# Define heatmap first\n",
    "plt.subplots(figsize=(10, 7.5))\n",
    "heatmap = sns.heatmap(State_Matrix, fmt=\".2f\", linewidths=0.25, linecolor='black', cbar=False, cmap='rocket_r')\n",
    "heatmap.set_facecolor('black') \n",
    "# coloring_blocks(heatmap, oil_states=[(3,5),(4,6)], bump_states=[(13,15),(14,16)], \\\n",
    "#                 start_state=(2,2),end_state=(12,12))\n",
    "    \n",
    "\n",
    "# Plot the route from the start state to the end state.\n",
    "# This is just an example, you may want to keep pi* coordinates and actions in a different way\n",
    "path = [((1,1),'right'), ((3,4),'down'), ((4,4),'right'), ((4,5),'down'), \\\n",
    "        ((5,5),'right'), ((5,6),'down'), ((6,6),'right'), ((6,7),'down')]\n",
    "for state_cr, direction in path:\n",
    "    r = state_cr[0] # x_coordinate\n",
    "    c = state_cr[1] # y_coordinate\n",
    "\n",
    "    if direction == 'right':\n",
    "        plt.arrow(c + 0.5, r + 0.5, 0.8, 0, width=0.04, color='blue')   # Right\n",
    "    if direction == 'left':\n",
    "        plt.arrow(c + 0.5, r + 0.5, -0.8, 0, width=0.04, color='black')  # Left\n",
    "    if direction == 'up':\n",
    "        plt.arrow(c + 0.5, r + 0.5, 0, -0.8, width=0.04, color='black')  # Up\n",
    "    if direction == 'down':\n",
    "        plt.arrow(c + 0.5, r + 0.5, 0, 0.8, width=0.04, color='black')  # Down\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
